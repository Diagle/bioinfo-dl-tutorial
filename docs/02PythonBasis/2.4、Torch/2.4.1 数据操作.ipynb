{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d73b780",
   "metadata": {},
   "source": [
    "# 1 引言\n",
    "## 1.1 数据\n",
    "\n",
    "每个数据集由一个样本组成（example，sample）组成，大多数时候遵循独立同分布（independently and identically distributed，i.i.d）。样本有时也称为数据点（data point）或数据实例（data instance），通常由一组称为特征（features）或协变量（covariates）的属性组成。机器学习就是根据这些属性进行预测，监督学习中预测的是一个特殊的属性称为标签（label）或目标（target）。\n",
    "\n",
    "当每个样本的特征类别数量都相同时，其特征 向量是固定长度的，这个长度称为维度（dimensionality）。\n",
    "\n",
    "## 1.2、模型\n",
    "\n",
    "深度学习关注于功能强大的模型，这些模型由神经网络错综复杂的交织在一起，包含层层数据转换，因此称为深度学习（deep learning）。\n",
    "\n",
    "## 1.3 目标函数\n",
    "\n",
    "机器学习中，需要定义模型的优劣程度的度量，这个度量大多数时候是“可优化”的。称之为目标函数（objective function）。因为希望优化到最低点，所以也称为损失函数（loss function）。任务在预测数值时，常见的损失函数是平方误差（squared error）。在解决分类问题时，常见的目标函数时最小化错误率，但由于不可微性或复杂性难以直接优化。用以训练的数据称为训练数据集（train dataset）或训练集（train set），用以拟合模型参数。用以测试模型最终性能的数据集称为测试数据集（test dataset）或测试集（test set）。\n",
    "\n",
    "## 1.4 优化算法\n",
    "\n",
    "能够搜索出最佳参数最小化损失函数的算法，称为优化算法。深度学习中大多数的算法都是基于一种基本方法——梯度下降（gradient decent）。在每个步骤中会检查每个参数，对该参数进行少量变动损失会朝向何处移动。然后往减少损失的方向上优化参数。\n",
    "\n",
    "## 1.5 监督学习\n",
    "\n",
    "### 1.5.1 回归\n",
    "\n",
    "### 1.5.2 分类\n",
    "\n",
    "### 1.5.3 标记问题\n",
    "\n",
    "### 1.5.4 搜索\n",
    "\n",
    "### 1.5.5 推荐系统\n",
    "\n",
    "### 1.5.6 序列学习\n",
    "\n",
    "## 1.6 无监督学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157a25d1",
   "metadata": {},
   "source": [
    "# 2 数据操作\n",
    "\n",
    "## 2.1 入门\n",
    "\n",
    "张量表示一个由数值组成的数组，数组可能有多个维度。一个轴的张量对应向量（vector）；两个轴的对应于数学上的矩阵（matrix）；两个以上的轴没有特殊数学名称。\n",
    "\n",
    "张量中的每个值称为张量的元素（element）。除非额外指定，张量存储在内存中，基于CPU计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb7e433a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "torch.Size([12])\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# arange创建一个行向量x，包含0开始的前12个整数，默认为整型。\n",
    "x = torch.arange(12)\n",
    "print(x)\n",
    "\n",
    "# 通过访问张量的shape属性得到张量的形状\n",
    "print(x.shape) \n",
    "\n",
    "# 通过访问numel属性得到张量中元素的总数\n",
    "print(x.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a7e29a",
   "metadata": {},
   "source": [
    "想改变一个张量的形状而不改变数量和元素值，可以调用reshape函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06bf1a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "# x不会改变形状\n",
    "X = x.reshape(3,4)\n",
    "print(X)\n",
    "\n",
    "# 可以通过设置-1调用自动计算维度改变形状\n",
    "X_ = x.reshape(3,-1)\n",
    "print(X_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9c9666",
   "metadata": {},
   "source": [
    "使用全0、全1、其他常量或特定分布中随机采样的数字来初始化矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72c980c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n",
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n",
      "tensor([[ 1.5484, -1.1896, -0.8362,  0.1259],\n",
      "        [ 0.0781, -0.4032, -1.1912,  0.3621],\n",
      "        [-1.0024,  1.4979,  1.1101,  0.8369]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "# 全0矩阵\n",
    "print(torch.zeros((2,3,4)))\n",
    "\n",
    "# 全1矩阵\n",
    "print(torch.ones((2,3,4)))\n",
    "\n",
    "# 标准高斯分布采样\n",
    "print(torch.randn(3,4))\n",
    "\n",
    "# 包含数值的python列表（或嵌套列表）赋确定值。最外层的列表对应于轴0，内层的列表对应于轴1\n",
    "\n",
    "print(torch.tensor([[1,2,3],[4,5,6],[7,8,9]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765aecd2",
   "metadata": {},
   "source": [
    "## 2.2 运算符\n",
    "\n",
    "按元素运算，将二元运算符（$F:\\mathbb R^d,\\mathbb R^d\\to\\mathbb R^d$）应用于数组中每对位置对应的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d896b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "求和：tensor([2, 3, 4, 5])\n",
      "求差：tensor([-2, -1,  0,  1])\n",
      "求积：tensor([0, 2, 4, 6])\n",
      "求商：tensor([0.0000, 0.5000, 1.0000, 1.5000])\n",
      "求幂：tensor([0, 1, 4, 9])\n",
      "exp：tensor([ 1.0000,  2.7183,  7.3891, 20.0855])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "y = torch.tensor([2,2,2,2])\n",
    "print(f'求和：{x+y}') \n",
    "print(f'求差：{x-y}') \n",
    "print(f'求积：{x*y}') \n",
    "print(f'求商：{x/y}') \n",
    "print(f'求幂：{x**y}')\n",
    "print(f'exp：{torch.exp(x)}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa200ccc",
   "metadata": {},
   "source": [
    "线性代数运算，包括向量点积和矩阵乘法。\n",
    "\n",
    "可以把多个张量连结（cancatenate）叠起来形成一个更大的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3a32092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [ 1.,  2.,  3.,  4.],\n",
       "         [ 2.,  3.,  4.,  5.],\n",
       "         [ 5.,  6.,  7.,  8.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  1.,  2.,  3.,  4.],\n",
       "         [ 4.,  5.,  6.,  7.,  2.,  3.,  4.,  5.],\n",
       "         [ 8.,  9., 10., 11.,  5.,  6.,  7.,  8.]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12, dtype=torch.float32).reshape(3,4)\n",
    "Y = torch.tensor([[0,1,2,3],[2,3,4,5],[6,7,8,9]])\n",
    "\n",
    "torch.cat((x,y), dim=0),torch.cat((x,y), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86f92dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 逻辑运算符\n",
    "X == Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b753122d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66.)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对张量的所有元素求和\n",
    "X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c366e28f",
   "metadata": {},
   "source": [
    "## 2.3 广播\n",
    "某些情况下，即使形状不同，仍可通过调用广播机制（broadcasting mechanism）来执行按元素操作。工作方式如下：\n",
    "1. 复制元素来扩展一到两个数组，使得两个张量具有相同的形状\n",
    "2. 对生成的数组执行按元素操作\n",
    "大多数情况下验证数组中长度为1的轴进行广播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5baabaa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2]]),\n",
       " tensor([[0, 1]]),\n",
       " tensor([[0, 1],\n",
       "         [1, 2],\n",
       "         [2, 3]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3).reshape((3,1))\n",
    "b = torch.arange(2).reshape((1,2))\n",
    "a,b,a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3806d174",
   "metadata": {},
   "source": [
    " ## 2.4 索引和切片\n",
    "\n",
    "第一个元素的索引是0，最后一个元素缩影是-1；可以指定范围以包含第一个元素和最后一个元素的元素：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa2df504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8.,  9., 10., 11.]),\n",
       " tensor([[ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[-1],X[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "616c6a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  9.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将指定索引来将元素写入矩阵\n",
    "X[1,2] = 9\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "951b39f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 12., 12., 12.],\n",
       "        [12., 12., 12., 12.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ：代表沿着某个轴的所有元素\n",
    "X[0:2,:] = 12\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b09ae8",
   "metadata": {},
   "source": [
    "## 2.5 节省内存\n",
    "\n",
    "运行一些操作可能导致为新结果分配内存。id()函数提供了内存中引用对象的确切地址。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cedc02c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y before memory location:131888632813024\n",
      "Y after memory  location:131888642721344\n"
     ]
    }
   ],
   "source": [
    "before = id(Y)\n",
    "\n",
    "Y = X + Y\n",
    "\n",
    "print(f\"Y before memory location:{before}\\nY after memory  location:{id(Y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8efab50",
   "metadata": {},
   "source": [
    "通常这是不可取的，原因是：\n",
    "- 不想不必要地分配内存。通常情况下希望原地执行这些更新\n",
    "- 如果不原地更新，其他引用可能会指向旧的内存位置。\n",
    "使用Y[:]将操作的结果分配给先前分配的数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5eff85ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(Z):131888614863280\n",
      "id(Z):131888614863280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = torch.zeros_like(Y)\n",
    "print(f\"id(Z):{id(Z)}\")\n",
    "Z[:] = X + Y\n",
    "print(f\"id(Z):{id(Z)}\")\n",
    "\n",
    "# 后续使用中没有重复用到X，可以使用X[:] = X + Y 或X += Y减少内存开销\n",
    "before = id(X)\n",
    "X += Y\n",
    "id(X) == before\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e13f62",
   "metadata": {},
   "source": [
    "## 2.6 转换为其他python对象\n",
    "\n",
    "易于将torch定义的张量和numpy张量相互转换。两个数组共享底层内存，就地更改一个张量也会同时更改另一个张量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99212b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, torch.Tensor)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = X.numpy()\n",
    "B = torch.tensor(A)\n",
    "type(A),type(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bdf6ebd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.5000]), 3.5, 3.5, 3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([3.5])\n",
    "a,a.item(),float(a),int(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
