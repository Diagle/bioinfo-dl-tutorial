$$f^*=arg \min_{f\in\mathcal H}\frac{1}{N_s}\sum_{i=1}^{N_s}l(v_if(x_i),y_i)+\lambda R(T(D_s),T(D_t))$$

此类方法的核心是学习特征变换函数T

显式距离度量：
$$f^*=arg \min_{f\in\mathcal H}\frac{1}{N_s}\sum_{i=1}^{N_s}l(f(x_i),y_i)+\lambda D(D_s,D_t)$$
隐式距离度量：

$$f^*=arg \min_{f\in\mathcal H}\frac{1}{N_s}\sum_{i=1}^{N_s}l(f(x_i),y_i)+\lambda \rm{Metric}\it(D_s,D_t)$$

核心是寻求一种显式或隐式的距离度量，使得在此种距离度量下，两域的数据分布差异可以减小

## 最大均值差异法

最大均值差异(Maximum Mean Discrepancy，MMD)


## 基因度量学习的迁移方法

